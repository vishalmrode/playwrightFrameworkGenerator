'use strict';

var fsExtra = require('fs-extra');
var assert = require('node:assert/strict');
var undici = require('undici');
var createDeferred = require('../../async/createDeferred.js');
var createPromiseQueue = require('../../async/createPromiseQueue.js');
var retryOnFailure = require('../../async/retryOnFailure.js');
var config$1 = require('../../config.js');
var logger = require('../../logger.js');
var beginRecordingMultipartUpload = require('../../protocol/api/beginRecordingMultipartUpload.js');
var beginRecordingUpload = require('../../protocol/api/beginRecordingUpload.js');
var endRecordingMultipartUpload = require('../../protocol/api/endRecordingMultipartUpload.js');
var endRecordingUpload = require('../../protocol/api/endRecordingUpload.js');
var processRecording = require('../../protocol/api/processRecording.js');
var setRecordingMetadata = require('../../protocol/api/setRecordingMetadata.js');
var getUserAgent = require('../../session/getUserAgent.js');
var config = require('../config.js');
var types = require('../types.js');
var updateRecordingLog = require('../updateRecordingLog.js');
var uploadSourceMaps = require('./uploadSourceMaps.js');
var validateRecordingMetadata = require('./validateRecordingMetadata.js');

function _interopDefault (e) { return e && e.__esModule ? e : { default: e }; }

var assert__default = /*#__PURE__*/_interopDefault(assert);

const uploadQueue = createPromiseQueue.createPromiseQueue({ concurrency: 10 });
async function uploadRecording(client, recording, options) {
  logger.logInfo("UploadRecording:Started", { recordingId: recording.id });
  const { buildId, id, path } = recording;
  assert__default.default(path, "Recording path is required");
  const { multiPartUpload, processingBehavior } = options;
  const { size } = await fsExtra.stat(path);
  logger.logDebug(`Uploading recording ${recording.id} of size ${size}`, { recording });
  const { metadata, recordingData } = await validateRecordingMetadata.validateRecordingMetadata(recording);
  recording.uploadStatus = "uploading";
  try {
    if (multiPartUpload && size > config.multiPartMinSizeThreshold) {
      const { chunkSize, partLinks, recordingId, uploadId } = await beginRecordingMultipartUpload.beginRecordingMultipartUpload(
        client,
        {
          buildId,
          maxChunkSize: config.multiPartChunkSize,
          recordingId: id,
          recordingSize: size
        }
      );
      updateRecordingLog.updateRecordingLog(recording, {
        kind: types.RECORDING_LOG_KIND.uploadStarted,
        server: config$1.replayWsServer
      });
      await retryOnFailure.retryWithExponentialBackoff(
        () => setRecordingMetadata.setRecordingMetadata(client, { metadata, recordingData }),
        (error, attemptNumber) => {
          logger.logDebug(`Attempt ${attemptNumber} to set metadata failed`, { error });
        }
      );
      const partIds = await uploadRecordingFileInParts({
        chunkSize,
        partLinks,
        recordingPath: path
      });
      await endRecordingMultipartUpload.endRecordingMultipartUpload(client, { partIds, recordingId, uploadId });
    } else {
      const { recordingId, uploadLink } = await beginRecordingUpload.beginRecordingUpload(client, {
        buildId,
        recordingId: id,
        recordingSize: size
      });
      updateRecordingLog.updateRecordingLog(recording, {
        kind: types.RECORDING_LOG_KIND.uploadStarted,
        server: config$1.replayWsServer
      });
      await retryOnFailure.retryWithExponentialBackoff(
        () => setRecordingMetadata.setRecordingMetadata(client, { metadata, recordingData }),
        (error, attemptNumber) => {
          logger.logDebug(`Attempt ${attemptNumber} to set metadata failed`, { error });
        }
      );
      await uploadQueue.add(
        () => retryOnFailure.retryWithExponentialBackoff(
          () => uploadRecordingFile({
            recordingPath: path,
            size,
            url: uploadLink
          }),
          (error, attemptNumber) => {
            logger.logDebug(`Attempt ${attemptNumber} to upload failed`, { error });
            if (error.code === "ENOENT") {
              throw error;
            }
          }
        )
      );
      await endRecordingUpload.endRecordingUpload(client, { recordingId });
    }
  } catch (error) {
    updateRecordingLog.updateRecordingLog(recording, {
      kind: types.RECORDING_LOG_KIND.uploadFailed
    });
    logger.logError("UploadRecording:Failed", {
      error,
      recordingId: recording.id,
      buildId: recording.buildId
    });
    recording.uploadStatus = "failed";
    recording.uploadError = error;
    throw error;
  }
  logger.logDebug(`Uploaded ${size} bytes for recording {recording.id}`);
  if (recording.metadata.sourceMaps.length) {
    await uploadSourceMaps.uploadSourceMaps(client, recording);
    logger.logDebug(`Uploaded source maps for recording ${recording.id}`);
  }
  updateRecordingLog.updateRecordingLog(recording, {
    kind: types.RECORDING_LOG_KIND.uploadFinished,
    server: config$1.replayWsServer
  });
  logger.logInfo("UploadRecording:Succeeded", { recording: recording.id });
  recording.uploadStatus = "uploaded";
  switch (processingBehavior) {
    case "start-processing": {
      logger.logDebug(`Start processing recording ${recording.id} ...`);
      processRecording.processRecording(client, { recordingId: recording.id }).catch(() => {
      });
      break;
    }
    case "wait-for-processing-to-finish": {
      logger.logDebug(`Begin processing recording ${recording.id} ...`);
      try {
        await client.waitUntilAuthenticated();
        logger.logDebug(`Processing recording ${recording.id}`);
        updateRecordingLog.updateRecordingLog(recording, {
          kind: types.RECORDING_LOG_KIND.processingStarted
        });
        recording.processingStatus = "processing";
        await retryOnFailure.retryWithExponentialBackoff(
          () => processRecording.processRecording(client, { recordingId: recording.id }),
          (error, attemptNumber) => {
            logger.logDebug(`Processing failed after ${attemptNumber} attempts`, { error });
          }
        );
        updateRecordingLog.updateRecordingLog(recording, {
          kind: types.RECORDING_LOG_KIND.processingFinished
        });
        recording.processingStatus = "processed";
      } catch (error) {
        updateRecordingLog.updateRecordingLog(recording, {
          kind: types.RECORDING_LOG_KIND.processingFailed
        });
        recording.processingStatus = "failed";
      }
      break;
    }
  }
}
async function uploadRecordingFile({
  recordingPath,
  size,
  url
}) {
  const stream = fsExtra.createReadStream(recordingPath);
  await uploadRecordingReadStream(stream, { url, size });
}
async function uploadRecordingFileInParts({
  chunkSize,
  partLinks,
  recordingPath
}) {
  const { size: totalSize } = await fsExtra.stat(recordingPath);
  const abortController = new AbortController();
  const partsUploads = partLinks.map((url, index) => {
    return uploadQueue.add(async () => {
      try {
        return retryOnFailure.retryWithLinearBackoff(
          async () => {
            const partNumber = index + 1;
            const start = index * chunkSize;
            const end = Math.min(start + chunkSize, totalSize) - 1;
            logger.logDebug("Uploading part", {
              partNumber,
              start,
              end,
              totalSize,
              chunkSize
            });
            return uploadPart(
              { url, recordingPath, start, end, size: end - start + 1 },
              abortController.signal
            );
          },
          (error, attemptNumber, maxAttempts) => {
            let message = `Failed to upload part ${index + 1}`;
            if (attemptNumber < maxAttempts && error.code !== "ENOENT") {
              message += `; will be retried`;
            }
            logger.logError(message, { error });
            if (error.code === "ENOENT") {
              throw error;
            }
          }
        );
      } catch (error) {
        abortController.abort();
        throw error;
      }
    });
  });
  return Promise.all(partsUploads);
}
async function uploadPart({
  url,
  recordingPath,
  start,
  end,
  size
}, abortSignal) {
  logger.logInfo("UploadRecording:UploadPart:Started", {
    recordingPath,
    size,
    start,
    end
  });
  const stream = fsExtra.createReadStream(recordingPath, { start, end });
  try {
    const response = await uploadRecordingReadStream(stream, { url, size }, abortSignal);
    const etag = response.headers.get("etag");
    logger.logInfo("UploadRecording:UploadPart:Succeeded", {
      recordingPath,
      size,
      start,
      end,
      etag
    });
    assert__default.default(etag, "Etag has to be returned in the response headers");
    return etag;
  } catch (error) {
    logger.logError("UploadRecording:UploadPart:Failed", {
      recordingPath,
      size,
      start,
      end
    });
    throw error;
  }
}
async function uploadRecordingReadStream(stream, { url, size }, abortSignal) {
  abortSignal?.throwIfAborted();
  const streamError = createDeferred.createDeferred();
  const closeStream = () => stream.close();
  stream.on("error", streamError.reject);
  const userAgent = await getUserAgent.getUserAgent();
  try {
    const response = await Promise.race([
      undici.fetch(url, {
        headers: {
          "Content-Length": size.toString(),
          "User-Agent": userAgent,
          Connection: "keep-alive"
        },
        method: "PUT",
        body: stream,
        duplex: "half",
        signal: abortSignal
      }),
      streamError.promise
    ]);
    logger.logDebug("Fetch response received", { response });
    if (!response.ok) {
      const respText = await response.text();
      logger.logDebug(`Fetch response text: ${respText}`);
      throw new Error(
        `Failed to upload recording. Response was ${response.status} ${response.statusText}`
      );
    }
    return response;
  } finally {
    closeStream();
  }
}

exports.uploadRecording = uploadRecording;
